{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Getting Started","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:18:04.414679Z","iopub.execute_input":"2024-12-22T14:18:04.414971Z","iopub.status.idle":"2024-12-22T14:21:10.042427Z","shell.execute_reply.started":"2024-12-22T14:18:04.41493Z","shell.execute_reply":"2024-12-22T14:21:10.041032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading `Llama-3.2-11B-Vision-Instruct` Model","metadata":{}},{"cell_type":"code","source":"from unsloth import FastVisionModel\nimport torch\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n    load_in_4bit = True,\n    use_gradient_checkpointing = \"unsloth\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:21:10.043564Z","iopub.execute_input":"2024-12-22T14:21:10.043893Z","iopub.status.idle":"2024-12-22T14:22:09.865356Z","shell.execute_reply.started":"2024-12-22T14:21:10.043863Z","shell.execute_reply":"2024-12-22T14:22:09.864448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting Up the LoRA","metadata":{}},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers     = True, \n    finetune_language_layers   = True, \n    finetune_attention_modules = True,\n    finetune_mlp_modules       = True,\n    r = 16,           \n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3443,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:09.866954Z","iopub.execute_input":"2024-12-22T14:22:09.867269Z","iopub.status.idle":"2024-12-22T14:22:15.052799Z","shell.execute_reply.started":"2024-12-22T14:22:09.867239Z","shell.execute_reply":"2024-12-22T14:22:15.052046Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"philschmid/amazon-product-descriptions-vlm\", \n                       split = \"train[0:500]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:15.054277Z","iopub.execute_input":"2024-12-22T14:22:15.054588Z","iopub.status.idle":"2024-12-22T14:22:18.519355Z","shell.execute_reply.started":"2024-12-22T14:22:15.054556Z","shell.execute_reply":"2024-12-22T14:22:18.518698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:18.519993Z","iopub.execute_input":"2024-12-22T14:22:18.520275Z","iopub.status.idle":"2024-12-22T14:22:18.525108Z","shell.execute_reply.started":"2024-12-22T14:22:18.520235Z","shell.execute_reply":"2024-12-22T14:22:18.52445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[45][\"image\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:18.525868Z","iopub.execute_input":"2024-12-22T14:22:18.52612Z","iopub.status.idle":"2024-12-22T14:22:18.590805Z","shell.execute_reply.started":"2024-12-22T14:22:18.5261Z","shell.execute_reply":"2024-12-22T14:22:18.589824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[45][\"description\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:18.591682Z","iopub.execute_input":"2024-12-22T14:22:18.591924Z","iopub.status.idle":"2024-12-22T14:22:18.600049Z","shell.execute_reply.started":"2024-12-22T14:22:18.591901Z","shell.execute_reply":"2024-12-22T14:22:18.599379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Processing the Dataset","metadata":{}},{"cell_type":"code","source":"instruction = \"\"\"\nYou are an expert Amazon worker who is good at writing product descriptions. \nWrite the product description accurately by looking at the image.\n\"\"\"\n\n\ndef convert_to_conversation(sample):\n    conversation = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]},\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [{\"type\": \"text\", \"text\": sample[\"description\"]}],\n        },\n    ]\n    return {\"messages\": conversation}\n\n\npass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:18.602507Z","iopub.execute_input":"2024-12-22T14:22:18.602697Z","iopub.status.idle":"2024-12-22T14:22:18.614463Z","shell.execute_reply.started":"2024-12-22T14:22:18.60268Z","shell.execute_reply":"2024-12-22T14:22:18.613687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"converted_dataset = [convert_to_conversation(sample) for sample in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:18.616201Z","iopub.execute_input":"2024-12-22T14:22:18.616448Z","iopub.status.idle":"2024-12-22T14:22:19.53397Z","shell.execute_reply.started":"2024-12-22T14:22:18.616429Z","shell.execute_reply":"2024-12-22T14:22:19.533211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"converted_dataset[45]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:19.534801Z","iopub.execute_input":"2024-12-22T14:22:19.535053Z","iopub.status.idle":"2024-12-22T14:22:19.540166Z","shell.execute_reply.started":"2024-12-22T14:22:19.535028Z","shell.execute_reply":"2024-12-22T14:22:19.539538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Inference Before Fine-tuning","metadata":{}},{"cell_type":"code","source":"FastVisionModel.for_inference(model)  # Enable for inference!\n\nimage = dataset[45][\"image\"]\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction},\n        ],\n    }\n]\ninput_text = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\ninputs = tokenizer(\n    image,\n    input_text,\n    add_special_tokens=False,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\n\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(\n    **inputs,\n    streamer=text_streamer,\n    max_new_tokens=128,\n    use_cache=True,\n    temperature=1.5,\n    min_p=0.1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:22:19.541065Z","iopub.execute_input":"2024-12-22T14:22:19.541385Z","iopub.status.idle":"2024-12-22T14:23:10.969279Z","shell.execute_reply.started":"2024-12-22T14:22:19.541356Z","shell.execute_reply":"2024-12-22T14:23:10.968342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting Up Model for Fine-tuning","metadata":{}},{"cell_type":"code","source":"from unsloth import is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\n\nFastVisionModel.for_training(model)  # Enable for training!\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n    train_dataset=converted_dataset,\n    args=SFTConfig(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=30,\n        learning_rate=2e-4,\n        fp16=not is_bf16_supported(),\n        bf16=is_bf16_supported(),\n        logging_steps=5,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"none\",  # For Weights and Biases\n        remove_unused_columns=False,\n        dataset_text_field=\"\",\n        dataset_kwargs={\"skip_prepare_dataset\": True},\n        dataset_num_proc=4,\n        max_seq_length=2048,\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:23:10.971368Z","iopub.execute_input":"2024-12-22T14:23:10.971693Z","iopub.status.idle":"2024-12-22T14:23:11.076317Z","shell.execute_reply.started":"2024-12-22T14:23:10.971672Z","shell.execute_reply":"2024-12-22T14:23:11.075606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:23:11.077161Z","iopub.execute_input":"2024-12-22T14:23:11.077492Z","iopub.status.idle":"2024-12-22T14:23:11.082911Z","shell.execute_reply.started":"2024-12-22T14:23:11.07746Z","shell.execute_reply":"2024-12-22T14:23:11.082246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:23:11.083684Z","iopub.execute_input":"2024-12-22T14:23:11.083991Z","iopub.status.idle":"2024-12-22T14:52:24.61236Z","shell.execute_reply.started":"2024-12-22T14:23:11.083959Z","shell.execute_reply":"2024-12-22T14:52:24.61142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:24.613457Z","iopub.execute_input":"2024-12-22T14:52:24.614204Z","iopub.status.idle":"2024-12-22T14:52:24.620997Z","shell.execute_reply.started":"2024-12-22T14:52:24.614178Z","shell.execute_reply":"2024-12-22T14:52:24.620135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Inference After Fine-tuning","metadata":{}},{"cell_type":"code","source":"FastVisionModel.for_inference(model)  # Enable for inference!\n\nimage = dataset[45][\"image\"]\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction},\n        ],\n    }\n]\ninput_text = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\ninputs = tokenizer(\n    image,\n    input_text,\n    add_special_tokens=False,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\n\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(\n    **inputs,\n    streamer=text_streamer,\n    max_new_tokens=128,\n    use_cache=True,\n    temperature=1.5,\n    min_p=0.1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:24.621927Z","iopub.execute_input":"2024-12-22T14:52:24.622247Z","iopub.status.idle":"2024-12-22T14:52:35.873056Z","shell.execute_reply.started":"2024-12-22T14:52:24.622216Z","shell.execute_reply":"2024-12-22T14:52:35.872205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[60][\"image\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:35.874358Z","iopub.execute_input":"2024-12-22T14:52:35.874592Z","iopub.status.idle":"2024-12-22T14:52:35.907996Z","shell.execute_reply.started":"2024-12-22T14:52:35.874571Z","shell.execute_reply":"2024-12-22T14:52:35.907065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = dataset[60][\"image\"]\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction},\n        ],\n    }\n]\ninput_text = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\ninputs = tokenizer(\n    image,\n    input_text,\n    add_special_tokens=False,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\n\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(\n    **inputs,\n    streamer=text_streamer,\n    max_new_tokens=128,\n    use_cache=True,\n    temperature=1.5,\n    min_p=0.1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:35.908963Z","iopub.execute_input":"2024-12-22T14:52:35.909282Z","iopub.status.idle":"2024-12-22T14:52:44.986653Z","shell.execute_reply.started":"2024-12-22T14:52:35.909253Z","shell.execute_reply":"2024-12-22T14:52:44.985949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the Adopter, Model, and Tokenizer","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:44.987731Z","iopub.execute_input":"2024-12-22T14:52:44.988018Z","iopub.status.idle":"2024-12-22T14:52:45.37411Z","shell.execute_reply.started":"2024-12-22T14:52:44.987994Z","shell.execute_reply":"2024-12-22T14:52:45.373404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"llama_3.2_vision_amazon_product\") # Local saving\ntokenizer.save_pretrained(\"llama_3.2_vision_amazon_product\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:45.37497Z","iopub.execute_input":"2024-12-22T14:52:45.375339Z","iopub.status.idle":"2024-12-22T14:52:47.986287Z","shell.execute_reply.started":"2024-12-22T14:52:45.375314Z","shell.execute_reply":"2024-12-22T14:52:47.985441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(\n    \"kingabzpro/llama_3.2_vision_amazon_product\"\n)  # Online saving\ntokenizer.push_to_hub(\n    \"kingabzpro/llama_3.2_vision_amazon_product\"\n)  # Online saving","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T14:52:47.987104Z","iopub.execute_input":"2024-12-22T14:52:47.987391Z","iopub.status.idle":"2024-12-22T14:52:54.409508Z","shell.execute_reply.started":"2024-12-22T14:52:47.987369Z","shell.execute_reply":"2024-12-22T14:52:54.408842Z"}},"outputs":[],"execution_count":null}]}