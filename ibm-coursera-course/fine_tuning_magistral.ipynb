{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPeoUW7U2i7U"
      },
      "source": [
        "## Setting Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO0JXmlP2i7f"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U transformers==4.52.1\n",
        "%pip install -U datasets\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl\n",
        "%pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Aehh_r2i7j",
        "outputId": "9fbaebb9-adc7-4b1d-d191-0dd10ecf4e5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "## Loading the model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkA1Qe3o2i7p",
        "outputId": "8aaf8c8b-f80f-40b0-a623-57b68d2ca1c6",
        "colab": {
          "referenced_widgets": [
            "52f42b51e22d4b168a0343e36fc4ed84",
            "6383bf255f6f405eb605177459e18307",
            "42dce34d459947d78007780f7d50ddae",
            "452bc862a3fd4a83b7627c593be04a74",
            "6514106fe73841f3943c90196e0b30cd",
            "57ec4c9834e9490785b69c1507c3f5c6",
            "6c7a9a6174464fa0ba77a2ee9f2052e6",
            "8aeedf415da94536a8f5b6e74afc46ca",
            "c3ce1b3af88343a694f0531d7bfe7e6e",
            "ee24cc0b28ef4cfaac510fe723de76cc",
            "d266e5a6e06e49b6b52abfe7a8816bc6",
            "7ade7b276283460f988391fdf0024a29"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52f42b51e22d4b168a0343e36fc4ed84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/201k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6383bf255f6f405eb605177459e18307",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42dce34d459947d78007780f7d50ddae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/21.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "452bc862a3fd4a83b7627c593be04a74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6514106fe73841f3943c90196e0b30cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57ec4c9834e9490785b69c1507c3f5c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/165k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c7a9a6174464fa0ba77a2ee9f2052e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8aeedf415da94536a8f5b6e74afc46ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ce1b3af88343a694f0531d7bfe7e6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee24cc0b28ef4cfaac510fe723de76cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d266e5a6e06e49b6b52abfe7a8816bc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ade7b276283460f988391fdf0024a29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load tokenizer & model\n",
        "\n",
        "model_dir = \"unsloth/Magistral-Small-2506-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUlRnBPE2i7q",
        "outputId": "f29dc3d5-7f61-4592-b240-23ae25b5d8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jun 12 09:08:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:4A:00.0 Off |                    0 |\n",
            "| N/A   28C    P0             82W /  400W |   22943MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "## Loading and processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_73Noxa2i7t"
      },
      "outputs": [],
      "source": [
        "train_prompt_style = \"\"\"\n",
        "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMkYPQux2i7u"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for question, response in zip(inputs, outputs):\n",
        "        # Remove the \"Q:\" prefix from the question\n",
        "        question = question.replace(\"Q:\", \"\")\n",
        "\n",
        "        # Append the EOS token to the response if it's not already there\n",
        "        if not response.endswith(tokenizer.eos_token):\n",
        "            response += tokenizer.eos_token\n",
        "\n",
        "        text = train_prompt_style.format(question, response)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEeTJSd02i7v",
        "outputId": "0394ddf0-044c-43e4-8075-3b9d1cda6c3d",
        "colab": {
          "referenced_widgets": [
            "2f219d93afea44a980429a1c12a05ab2",
            "5a0747a3279e40ca99c8d7017a21bae2",
            "0e04ffccdde84b6ba60c85ccafc6780a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f219d93afea44a980429a1c12a05ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "instruction-dataset-w-reasoning2.json:   0%|          | 0.00/7.26M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a0747a3279e40ca99c8d7017a21bae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/3702 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e04ffccdde84b6ba60c85ccafc6780a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3702 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
            "### Question:\n",
            "A research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood. A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease. A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance. When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity. Which of the following additional analyses would most likely address the concerns about this study's design?? \n",
            "{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\n",
            "\n",
            "### Response:\n",
            "<analysis>\n",
            "\n",
            "This is a question about assessing the validity of a prospective cohort study. The study found an association between childhood diet and cardiovascular disease in adulthood. The peer reviewer is concerned that the researchers did not discuss the validity of the study design. \n",
            "\n",
            "To address concerns about validity in a prospective cohort study, we need to consider potential confounding factors that could influence the results. The additional analysis suggested should help control for confounding.\n",
            "</analysis>\n",
            "<answer>\n",
            "D: Stratification\n",
            "</answer></s>\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"mamachang/medical-reasoning\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "print(dataset[\"text\"][10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuuak5Nl2i7w"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTyCRKiG2i7x"
      },
      "source": [
        "## Model inference before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8Nf5nxy2i7x"
      },
      "outputs": [],
      "source": [
        "inference_prompt_style = \"\"\"\n",
        "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<analysis>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDfQbAxq2i7y",
        "outputId": "690a4454-ae50-49b2-bb81-d971baf725a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<analysis>\n",
            " to 5 years old, and their diets are recorded for 1 year. The participants are then assessed 20 years later for cardiovascular disease. A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance. A peer reviewer comments that the researchers did not discuss the study's validity.\n",
            "\n",
            "The key issue here is the validity of the study. Validity refers to whether the study measures what it claims to measure and whether the findings are generalizable. In a prospective cohort study, validity is particularly important because it relies on self-reported data (diet) and long-term follow-up (20 years).\n",
            "\n",
            "The most critical aspect of validity in this context is ensuring that the observed association is not due to confounding factors (e.g., other dietary habits, physical activity levels, or socioeconomic status). To address this, the researchers should discuss how they controlled for potential confounders.\n",
            "\n",
            "Among the options:\n",
            "- **Blinding (A)**: Not directly relevant here because blinding is more important for preventing bias in intervention studies (e.g., clinical trials) rather than observational studies like this one.\n",
            "- **Crossover (B)**: This is a study design where participants switch between treatments. Not applicable here since this is a long-term observational study.\n",
            "- **Matching (C)**: This involves pairing participants based on certain characteristics (e.g., age, sex) to control for confounding. While matching could be useful, it is not the most comprehensive way to address validity in this case.\n",
            "- **Stratification (D)**: This involves dividing participants into subgroups (strata) based on certain characteristics (e.g., income level, physical activity) and analyzing outcomes within each stratum. This could help identify if the association varies across subgroups, which is useful for understanding effect modification, but it doesn’t directly address general validity concerns.\n",
            "- **Randomization (E)**: Randomization is the gold standard for controlling for confounding in experimental studies (e.g., randomized controlled trials). However, this study is observational (a cohort study), so randomization is not feasible. But the reviewer’s comment about validity likely refers to whether the study design and analysis appropriately controlled for confounding and bias.\n",
            "\n",
            "The best option to address validity in this observational study would be to discuss how the researchers accounted for confounding factors (e.g., through statistical adjustment or matching). However, since matching (C) is explicitly listed as an option, and it is a method to control for confounding by matching participants on key variables, this is the most relevant choice here.\n",
            "\n",
            "But wait—the question asks which\n"
          ]
        }
      ],
      "source": [
        "question = dataset[10]['input']\n",
        "question = question.replace(\"Q:\", \"\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,                           # Scaling factor for LoRA\n",
        "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
        "    r=64,                                    # Rank of the LoRA update matrices\n",
        "    bias=\"none\",                             # No bias reparameterization\n",
        "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],  # Target modules for LoRA\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "e816d9293b8c4edebdb2922b8bfb59c2",
            "50447f3fd77f4173833cc578d3a40f4d",
            "c09e499d283b413c96ae03b06ceebdc4",
            "7932c35fc1b74bc59b7267f5e1809da4",
            "fced3c7e105a48538afcb10ca50713cc",
            "5b38e11c297849cd9cf39e9917ea2786",
            "532e5569f0c94129ba8287650091f595",
            "36eea72d58df4da59600e6c85e36895e",
            "0cf71963c58343ba8dd7626eafd8e4fe",
            "1dfec2a3a94444fbb5132fe40c9a4c44",
            "cebc85512fbe42b3998c1d3df7a969e7",
            "63eccf4453514689a22f5e2b6c8c15a5",
            "0955a1a2e5cb480ab946e64d46af9bc9",
            "94cdc9f9960d4042a7fbd8a9398b24ed",
            "3357ce04955c45cc866ad7ccf79df303"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "a446001c-38e9-44b6-beb8-c9e0945ad03f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63eccf4453514689a22f5e2b6c8c15a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/3702 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0955a1a2e5cb480ab946e64d46af9bc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/3702 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94cdc9f9960d4042a7fbd8a9398b24ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/3702 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3357ce04955c45cc866ad7ccf79df303",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/3702 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "# Training Arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"Magistral-Medical-Reasoning\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=0.2,\n",
        "    warmup_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "d6d462bd-17d2-438e-8e79-073e647cd912"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1851' max='1851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1851/1851 1:09:25, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>0.908700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1113</td>\n",
              "      <td>0.880900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1484</td>\n",
              "      <td>0.854700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1851, training_loss=0.8885096586568494, metrics={'train_runtime': 4168.7115, 'train_samples_per_second': 0.888, 'train_steps_per_second': 0.444, 'total_flos': 2.26971228206592e+17, 'train_loss': 0.8885096586568494})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc, torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHgNDses2i72"
      },
      "source": [
        "## Model inference after fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql9IRyEm2i73",
        "outputId": "71e2471a-bb6a-4e2e-c9e7-f2b2a2be1c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<analysis>\n",
            "Analysis:\n",
            "\n",
            "This is a prospective cohort study looking at the relationship between childhood diet and cardiovascular disease in adulthood. The key issue with the validity of this study is confounding. The researchers did not account for other factors that could influence cardiovascular disease risk in adulthood besides childhood diet. \n",
            "\n",
            "To address confounding, the researchers should have stratified the analysis by potential confounders like family history, physical activity levels, and smoking status. This would allow them to see if the relationship between diet and disease persists even when accounting for these other variables. \n",
            "\n",
            "Blinding, crossover, matching, and randomization do not address the main validity concern in this study design.\n",
            "</analysis>\n",
            "<answer>\n",
            "D: Stratification\n",
            "</answer>\n"
          ]
        }
      ],
      "source": [
        "question = dataset[10]['input']\n",
        "question = question.replace(\"Q:\", \"\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [inference_prompt_style.format(question,) + tokenizer.eos_token],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIiQom9r2i74",
        "outputId": "ba6fec10-2ce6-43a9-fbd1-22901da32df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<analysis>\n",
            "\n",
            "This is a question about assessing the validity of a prospective cohort study. The study found an association between childhood diet and cardiovascular disease in adulthood. The peer reviewer is concerned that the researchers did not discuss the validity of the study design. \n",
            "\n",
            "To address concerns about validity in a prospective cohort study, we need to consider potential confounding factors that could influence the results. The additional analysis suggested should help control for confounding.\n",
            "</analysis>\n",
            "<answer>\n",
            "D: Stratification\n",
            "</answer>\n"
          ]
        }
      ],
      "source": [
        "print(dataset[10]['output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5dwPN9M2i74",
        "outputId": "0bb75d1e-6d3f-4943-d4ae-a0c03f2a6e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<analysis>\n",
            " analysis>\n",
            "\n",
            "This is a clinical vignette describing a 55-year-old man with burning and shooting pain in his feet and lower legs that worsens at night. He has a history of type 2 diabetes mellitus and hypertension. \n",
            "\n",
            "The key findings are:\n",
            "- Burning and shooting pain in feet and lower legs \n",
            "- Pain worsens at night\n",
            "- History of type 2 diabetes mellitus\n",
            "\n",
            "This presentation is most consistent with diabetic peripheral neuropathy. The pain distribution, timing, and history of diabetes point towards a distal symmetric sensorimotor polyneuropathy as the etiology. The other options can be ruled out based on the clinical presentation.\n",
            "</analysis>\n",
            "<answer>\n",
            "D: Distal symmetric sensorimotor polyneuropathy\n",
            "</answer>\n"
          ]
        }
      ],
      "source": [
        "question = dataset[100]['input']\n",
        "question = question.replace(\"Q:\", \"\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRYhFU_A2i75",
        "outputId": "4f928612-8138-47ca-9561-45335e85729c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<analysis>\n",
            "\n",
            "This patient has a history of type 2 diabetes mellitus and is experiencing burning and shooting pains in his feet and lower legs that are worse at night and have progressed over the past 6 months. This presentation is most consistent with distal symmetric sensorimotor polyneuropathy, a type of diabetic neuropathy that affects the distal extremities in a length-dependent pattern. Autonomic neuropathy, cranial nerve neuropathy, and radiculopathy would not explain the symmetric distal distribution. Isolated peripheral neuropathy would not be expected in the setting of longstanding diabetes.\n",
            "</analysis>\n",
            "<answer>\n",
            "D: Distal symmetric sensorimotor polyneuropathy\n",
            "</answer>\n"
          ]
        }
      ],
      "source": [
        "print(dataset[100]['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### Saving finetuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "d175f7db0c2d4531bcb8d747a4a6a2a2",
            "8e022663a0074f9ba439ac9c8b731936",
            "79fb70e5e6aa4f258b2613687e59bc4b"
          ]
        },
        "id": "upcOlWe7A1vc",
        "outputId": "f82ddff3-162f-4281-b7b5-cab363a89a05"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d175f7db0c2d4531bcb8d747a4a6a2a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading...:   0%|          | 0.00/1.48G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e022663a0074f9ba439ac9c8b731936",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79fb70e5e6aa4f258b2613687e59bc4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading...:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/kingabzpro/Magistral-Small-Medical-QA/commit/093367ec7f9fbb733572d96dbde6a1430f0312a0', commit_message='Upload tokenizer', commit_description='', oid='093367ec7f9fbb733572d96dbde6a1430f0312a0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kingabzpro/Magistral-Small-Medical-QA', endpoint='https://huggingface.co', repo_type='model', repo_id='kingabzpro/Magistral-Small-Medical-QA'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_model_name = \"kingabzpro/Magistral-Small-Medical-QA\"\n",
        "\n",
        "trainer.model.push_to_hub(new_model_name)\n",
        "trainer.processing_class.push_to_hub(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtNvUjFp2i78"
      },
      "source": [
        "## Loading the Adopter and testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfGYz_N42i78"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OMb2eRZ2i79",
        "outputId": "aaebf476-2409-466b-e30f-c71835ab4912",
        "colab": {
          "referenced_widgets": [
            "2176e62f67d74fe7a809ce71baaeae75",
            "e444757f63eb4e2bbc6d1e3ec5148a4d",
            "b0ef5007ec1247ffa2a2fcaa5af9d5d0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2176e62f67d74fe7a809ce71baaeae75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e444757f63eb4e2bbc6d1e3ec5148a4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/870 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0ef5007ec1247ffa2a2fcaa5af9d5d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/1.48G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Base model\n",
        "base_model_id = \"unsloth/Magistral-Small-2506-bnb-4bit\"\n",
        "\n",
        "# Your fine-tuned LoRA adapter repository\n",
        "lora_adapter_id = \"kingabzpro/Magistral-Small-Medical-QA\"\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Attach the LoRA adapter\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_adapter_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kNFoTW52i7-",
        "outputId": "65027e03-cfb0-44da-ee53-d85a13ccb940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<analysis>\n",
            "\n",
            "Analysis:\n",
            "\n",
            "This is a prospective cohort study looking at the relationship between childhood diet and cardiovascular disease in adulthood. The peer reviewer is concerned about the validity of the study's findings. To address concerns about validity in a prospective cohort study, we need to consider potential confounding factors and selection bias. \n",
            "\n",
            "Choice A, blinding, is not relevant since this is an observational study, not a clinical trial. \n",
            "\n",
            "Choice B, crossover, is also not applicable since this is a cohort study.\n",
            "\n",
            "Choice C, matching, could help control for confounding if patients were matched on relevant factors. However, the question does not indicate matching was done.\n",
            "\n",
            "Choice D, stratification, could help control for confounding by stratifying by key variables. This is a reasonable option.\n",
            "\n",
            "Choice E, randomization, is the best option. Randomizing patients to different diets would help control for confounding and selection bias. Randomization is the gold standard for controlling confounding in observational studies.\n",
            "</analysis>\n",
            "<answer>\n",
            "E: Randomization\n",
            "</answer>\n"
          ]
        }
      ],
      "source": [
        "# Inference example\n",
        "prompt = \"\"\"\n",
        "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
        "\n",
        "### Question:\n",
        "A research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood.\n",
        "A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease.\n",
        "A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance.\n",
        "When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity.\n",
        "Which of the following additional analyses would most likely address the concerns about this study's design?\n",
        "{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\n",
        "### Response:\n",
        "<analysis>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [prompt + tokenizer.eos_token],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cf71963c58343ba8dd7626eafd8e4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dfec2a3a94444fbb5132fe40c9a4c44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36eea72d58df4da59600e6c85e36895e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50447f3fd77f4173833cc578d3a40f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b38e11c297849cd9cf39e9917ea2786",
            "placeholder": "​",
            "style": "IPY_MODEL_532e5569f0c94129ba8287650091f595",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=12): 100%"
          }
        },
        "532e5569f0c94129ba8287650091f595": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b38e11c297849cd9cf39e9917ea2786": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7932c35fc1b74bc59b7267f5e1809da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dfec2a3a94444fbb5132fe40c9a4c44",
            "placeholder": "​",
            "style": "IPY_MODEL_cebc85512fbe42b3998c1d3df7a969e7",
            "value": " 19252/19252 [00:50&lt;00:00, 416.33 examples/s]"
          }
        },
        "c09e499d283b413c96ae03b06ceebdc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36eea72d58df4da59600e6c85e36895e",
            "max": 19252,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cf71963c58343ba8dd7626eafd8e4fe",
            "value": 19252
          }
        },
        "cebc85512fbe42b3998c1d3df7a969e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e816d9293b8c4edebdb2922b8bfb59c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50447f3fd77f4173833cc578d3a40f4d",
              "IPY_MODEL_c09e499d283b413c96ae03b06ceebdc4",
              "IPY_MODEL_7932c35fc1b74bc59b7267f5e1809da4"
            ],
            "layout": "IPY_MODEL_fced3c7e105a48538afcb10ca50713cc"
          }
        },
        "fced3c7e105a48538afcb10ca50713cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}