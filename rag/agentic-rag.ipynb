{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bce86d-1039-4b01-a448-370d5440753d",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "The notebook is adopted form https://www.ibm.com/think/tutorials/agentic-rag to run locally with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3345e22-e057-4691-b2b1-e8a0121c4c14",
   "metadata": {},
   "source": [
    "## Step 1. Setup Ollama & ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84c0a6-66c3-4530-a13d-9bab62214956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb langgraph langchain_chroma "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f83a35-32d3-4d6a-a468-83bced44d54a",
   "metadata": {},
   "source": [
    "## Step 2. Import necessary lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a610f4ab-c69e-41f3-8769-904f00cf5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser, ReActJsonSingleInputOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str, format_log_to_messages\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326a516-1a9f-4b89-8996-abad9ccfd769",
   "metadata": {},
   "source": [
    "## Step 3. Setup LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6852d815-cb0f-4675-a6f5-4fdfec70f87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.1:8b', temperature=0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "model_name =  \"llama3.1:8b\" #\"qwen2.5:7b\"\n",
    "\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993447f-669d-42d2-a126-3181ff3b1f30",
   "metadata": {},
   "source": [
    "## Step 4. Setup Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ccfca4-df88-4b0a-9f5d-ea0aa7faebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the {query} accurately. If you do not know the answer, simply say you do not know.\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c449d3-17f2-4c62-b122-96f156d5095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52437d9e-4f43-4abf-bcfc-a090dd0f37aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sport played at the US Open is tennis.', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-07-30T13:48:33.8306724Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1918515200, 'load_duration': 190334600, 'prompt_eval_count': 38, 'prompt_eval_duration': 417464500, 'eval_count': 11, 'eval_duration': 1306014300, 'model_name': 'llama3.1:8b'}, id='run--2c1ee129-d7d4-4597-adbc-5e7c7a50675b-0', usage_metadata={'input_tokens': 38, 'output_tokens': 11, 'total_tokens': 49})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": 'What sport is played at the US Open?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a7a4e-c932-434e-8079-6bc2ac010b60",
   "metadata": {},
   "source": [
    "The agent successfully responded to the basic query with the correct answer. In the next step of this tutorial, we will be creating a RAG tool for the agent to access relevant information about IBM's involvement in the 2024 US Open. As we have covered, traditional LLMs cannot obtain current information on their own. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd50c026-1746-49f1-b016-3bb6c4794c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have information about future events, but I can suggest checking official sources for updates on the location of the 2024 US Open Tennis Championship.\", additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-07-30T13:48:38.9163311Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5058050700, 'load_duration': 208389300, 'prompt_eval_count': 40, 'prompt_eval_duration': 207248100, 'eval_count': 32, 'eval_duration': 4637390400, 'model_name': 'llama3.1:8b'}, id='run--364b58b0-32f5-41b1-bf3a-b1eb254c1d39-0', usage_metadata={'input_tokens': 40, 'output_tokens': 32, 'total_tokens': 72})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": 'Where was the 2024 US Open Tennis Championship?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8277a1-f006-425b-bfae-99d24063ed01",
   "metadata": {},
   "source": [
    "Evidently, the LLM is unable to provide us with the relevant information. The training data used for this model contained information prior to the 2024 US Open and without the appropriate tools, the agent does not have access to this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b5dee-b2e7-42b6-ade8-e01851259747",
   "metadata": {},
   "source": [
    "## Step 5. Establish the knowledge base and retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5600f50-e97e-4426-b7db-cb19d52e9dd1",
   "metadata": {},
   "source": [
    "The first step in creating the knowledge base is listing the URLs we will be extracting content from. In this case, our data source will be collected from our online content summarizing IBM’s involvement in the 2024 US Open. The relevant URLs are established in the urls  list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d62a8bb-3275-406e-9307-0e496c8b40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://www.ibm.com/case-studies/us-open',\n",
    "    'https://www.ibm.com/sports/usopen',\n",
    "    'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement',\n",
    "    'https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebade21c-b0da-4289-83a2-30231e4f97c9",
   "metadata": {},
   "source": [
    "Next, load the documents using LangChain WebBaseLoader  for the URLs we listed. We'll also print a sample document to see how it loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c6d096-a85d-4ff7-8aee-ee98e2825a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "#docs_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd01eeb-fd5e-45b1-9c50-15868a1a6dcd",
   "metadata": {},
   "source": [
    "Once the text splitter is initiated, we can apply it to our docs_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0527e2d3-bb5b-4a59-be01-95e91723c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72ec23-604d-4a22-996e-8d57416267d8",
   "metadata": {},
   "source": [
    "Let's initiate embeddings as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab3796a-8bec-4f31-a1a4-0fc7d45009e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6d6c4-b8e6-4ce2-8a70-360d4a9d2da5",
   "metadata": {},
   "source": [
    "In order to store our embedded documents, we will use Chroma DB, an open source vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b9be9d-57e8-4078-846a-1ea7a53c8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = Chroma.from_documents(\n",
    "#     documents=doc_splits,\n",
    "#     collection_name=\"agentic-rag-chroma\",\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./chroma_langchain_db\",\n",
    "# )\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"agentic-rag-chroma\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4017c29-4518-48d1-b9a8-5d5a9020205a",
   "metadata": {},
   "source": [
    "To access information in the vector store, we must set up a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18617d28-30cf-49c4-9e7e-66ac38753af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8435a3c-0c7f-4750-b016-d5056e9750c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3bf1e951-aab8-4faa-a03e-8271488a7015', metadata={'title': 'US Open: IBM and AI Can Help Engage Tennis Fans ', 'language': 'en-us', 'description': '', 'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement'}, page_content='The new, immersive fan content seems likely to continue evolving in the future. Even in a regular year, USTA’s Corio said, 10 million people use the site during the U.S. Open. With the right engagement, she said, that number can grow. “If we see that any of the technology really has traction, that will provide the fuel we need.”'),\n",
       " Document(id='e7a27a93-d4eb-4624-be38-5365af86eed4', metadata={'description': '', 'title': 'US Open: IBM and AI Can Help Engage Tennis Fans ', 'language': 'en-us', 'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement'}, page_content='Match Insights used Watson Natural Language Processing to pore over more than a hundred thousand written articles to glean the most relevant facts and insights.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\n",
    "    query='Where was the 2024 US Open Tennis Championship?',\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da7cf9-ab9e-4cfe-9890-52c60708dab5",
   "metadata": {},
   "source": [
    "## Step 6. Define the agent's RAG tool\n",
    "Let's define the retrieve()  tool our agent will be using. This tool's only parameter is the user query. The tool description is also noted to inform the agent of the use of the tool. This way, the agent knows when to call this tool. This tool can be used by the agentic RAG system for routing the user query to the vector store if it pertains to IBM’s involvement in the 2024 US Open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81ff4893-bae8-43c6-991b-97d52322c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.tools import ToolException, StructuredTool\n",
    "\n",
    "@tool\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=10)\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "tools = [retrieve]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64acd04-a648-4010-b230-753c2fcb69f9",
   "metadata": {},
   "source": [
    "## Step 7. Establish the prompt template\n",
    "\n",
    "Next, we will set up a new prompt template to ask multiple questions. This template is more complex. It is referred to as a structured chat prompt and can be used for creating agents that have multiple tools available. In our case, the tool we are using was defined in Step 6. The structured chat prompt will be made up of a system_prompt , a human_prompt  and our RAG tool.\n",
    "\n",
    "First, we will set up the system_prompt . This prompt instructs the agent to print its \"thought process,\" which involves the agent's subtasks, the tools that were used and the final output. This gives us insight into the agent's function calling. The prompt also instructs the agent to return its responses in JSON Blob format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdcaa1fd-6e9f-4682-a061-f1fe537caff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibagimon\\miniconda3\\envs\\aiml\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccddf4b3-43bd-41b7-982f-90176fe84ab6",
   "metadata": {},
   "source": [
    "In the following code, we are establishing the human_prompt . This prompt tells the agent to display the user input followed by the intermediate steps taken by the agent as part of the agent_scratchpad ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832bead-033b-48f8-aa66-9beed7370b8f",
   "metadata": {},
   "source": [
    "Now, lets finalize our prompt template by adding the tool names, descriptions and arguments using a partial prompt template. This allows the agent to access the information pertaining to each tool including the intended use cases. This also means we can add and remove tools without altering our entire prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eb072d7-5acb-404b-8885-860e5a1a461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f4140-03c5-4682-bdc9-7dec43adfead",
   "metadata": {},
   "source": [
    "## Step 8. Set up the agent's memory and chain\n",
    "An important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's ConversationBufferMemory()  as a means of memory storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ae286-d652-4838-b212-2b00fd83af2a",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our agent's scratchpad, memory, prompt and the LLM. The AgentExecutor  class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ffbd893-c0b3-4d87-afec-2eedae3fad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def search(state: State):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    print(state[\"question\"])\n",
    "    \n",
    "    result = retrieve(state[\"question\"])\n",
    "\n",
    "    print(f\"Retrieved {len(result)} records\")\n",
    "    \n",
    "    return {\"context\": result}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = agent_prompt.invoke({\n",
    "        \"input\": state[\"question\"],\n",
    "        \"agent_scratchpad\": docs_content\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "tools = [retrieve]\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([search, generate])\n",
    "graph_builder.add_edge(START, \"search\")\n",
    "graph_builder.add_edge(\"search\", END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31b4a0-22db-4d6a-a2ac-a3a67073e70d",
   "metadata": {},
   "source": [
    "## Step 9. Generate responses with the agentic RAG system\n",
    "We are now able to ask the agent questions. Recall the agent's previous inability to provide us with information pertaining to the 2024 US Open. Now that the agent has its RAG tool available to use, let's try asking the same questions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bee4d7-71f0-46e5-896e-0090879e995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was the 2024 US Open Tennis Championship?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibagimon\\AppData\\Local\\Temp\\ipykernel_15288\\1361847424.py:18: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = retrieve(state[\"question\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 records\n"
     ]
    }
   ],
   "source": [
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"local\"}}\n",
    "\n",
    "query='Where was the 2024 US Open Tennis Championship?'\n",
    "\n",
    "response = graph.invoke({\"question\": query}, config=config)\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
